# Speech Module
The speech module provides a extendable Speech-To-Text (Speech Recognition) and Text-To-Speech (Speech Synthesis) functionalities to Unity program on Windows Standalone, UWP, and Android platforms.

## Components
The speech module consists of three components: speech recognizers, speech synthesizers, and the `SpeechProvider`. You can implement your own recognizers and synthesizers if needed. All licenses of third-party libraries can be found in `THIRD-PARTY-NOTICES` under the `Third Party Plugins` folder.

### Speech Recognizer
All speech recognizers should implement the `ISpeechRecognizer` interface and realize its `StartRecordingAsync()` and `StopRecordingAsync()` methods, `Language` and `IsApplicable` properties, and `OnRecognitionResultReceived` event. It should also inherits ``MonoBehavior``. There are two pre-implemented instances in the module:
  - `AzureSpeechRecognizer`, which uses the [Azure Congitive Service](https://azure.microsoft.com/en-us/services/cognitive-services/#overview) of Microsoft. It provides two modes: SingleShot and Continuous. In the SingleShot mode, it stops automatically when it detects silence, while in the Continuous mode, user must stop it manually. You needs a subscribtion key and service region to use the Azure service, and of course also internet conection. To use this recognizer, one needs the [SpeechSDK](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/quickstarts/setup-platform?pivots=programming-language-csharp&tabs=windows%2Cubuntu%2Cunity%2Cjre%2Cmaven%2Cbrowser%2Cmac%2Cpypi).
  - `NativeSpeechRecognizer`, which is neural-network based and can run offline on the device. It uses the [Vosk](https://alphacephei.com/vosk/index) library. Since it is neural-network based, one must specify neural-network models. They can be downloaded [here](https://alphacephei.com/vosk/models). It is suggested to download the small models, which are typically around 40 to 50MB. The models must be placed under `Assets/StreamingAssets` folder. On the inspector view, you need to specify the path of the model. If the model is placed under the `StreamingAssets` folder, the path is only the name of the model with ".zip" at the end. Basically you can add any language that has a model.

The Speech SDK is not included in the package, you need to import them. See the next chapter for detail.
### Speech Synthesizer
All speech synthesizer should implement the `ISpeechSyntheizer` interface and realize its `StartSynthesizingAndSpeakingAsync()` method, `Language`, `IsApplicable` and `OutputForm` properties, and the `OnSynthesisResultReceived` event. The `OutputForm` has two values: `To Speaker` and `As Byte Stream`. Considering some APIs allow developers get the raw byte stream, we suggest you to use `As Byte Stream` if you need a spatial sound setting. In this case, the stream will be converted to an `Audio Clip` and played by an `Audio Source` on the attached `GameObject`. It is useful especially when you develop an agent, since the spatial sound make it more human-like. However, if the API that you want to call don't support this, you can then neglect this property, the `SpeechProvider` would take care of it.

Again, there are two implemented instances:
- `AzureSpeechSynthesizer`, which works similar to the `AzureSpeechRecognizer`.
- `NativeSpeechSynthesizer`, which is an offline synthesizer. For Windows Standalone, it uses the [Microsoft Speech API (SAPI)](https://docs.microsoft.com/en-us/previous-versions/windows/desktop/ee125663(v=vs.85)) through the `interop.speechlib.dll`. For UWP, it uses the `Windows.Media.SpeechSynthesis` API through the `TextToSpeechUWP` script, which is a slightly modified version of the [`TextToSpeech` script in the `MixedRealityToolkit` of Microsoft](https://github.com/microsoft/MixedRealityToolkit-Unity/blob/main/Assets/MRTK/SDK/Features/Audio/TextToSpeech.cs). For Andorid, it uses the scripts and Android plugins from the GitHub repository [nir-takemi/UnityTTS](https://github.com/nir-takemi/UnityTTS). The native synthesizer only supports English on all platforms.
  
All third-party libraries are not included, you need to import them. See the next chapter for detail.

### Speech Provider
The `SpeechProvider` requires at least one `ISpeechRecognizer` and one `ISpeechSynthesizer` on the same `GameObject`. The ones with higher priorities should be placed on top of other recognizers and synthesizers ON the inspector. It manages the `ISpeechRecognizer` and `ISpeechSynthesizer` and exposes their functionalities to users. So you only need to re-implement your own `ISpeechRecognizer` and `ISpeechSynthesizer` if needed, and don't need to care about the user-interaction aspects for each of them. There maybe also other settings (SerializeField) on the recognizers and synthesizers. In case of the selected recognizer or synthesizer is not applicable by checking their `IsApplicable` property, it would automatically find another applicable one. For synthesizers, it repeats the synthesis for the given text again. However, for recognizers, users must repeat what they said since the audio data is not buffered on the device. Moreover, by settings its properties and subcribing its events, the values would be propagated to all recognizers and synthesizers, so you don't need to set them one by one. 

## Import the Libraries for Pre-implemented Recognizers and Synthesizers
In order to reduce the package size and not to force users to download external resources that they will even not use, the third-party libraries of the pre-implemented recognizers and synthesizers must be imported manually and some custom scripting symbols are defined for them, so that developers who use other modules in the i5 Toolkit but not the speech module don't need to download those resources. Note that the scripts themselves for the recognizers and synthesizers are contained in the package.

To import the above introduced `AzureSpeechRecognizer/Synthesizer` and `NativeSpeechRecognizer/Synthesizer`, you need to navigate to the _i5 Toolkit - Import Speech Module_ on the menu bar at the top of Unity Editor. By clicking on a recognizer/synthesizer, an importer will automatically download all resources required and import them, then it will set the corresponding custom scripting symbol. All custom scripting symbols used here are:
- I5_TOOLKIT_USE_AZURE_SPEECH_RECOGNIZER
- I5_TOOLKIT_USE_AZURE_SPEECH_SYNTHESIZER
- I5_TOOLKIT_USE_NATIVE_SPEECH_RECOGNIZER
- I5_TOOLKIT_USE_NATIVE_SPEECH_SYNTHESIZER

During the importing process, the importer will first download the required resources. A progress bar will be displayed on top of the editor window. However, you can still do other things during the download process. After that, the importer will import the downloaded package (except for the Vosk neural network models). There will be no pop-up window for importing so it will be done automatically. The imported packages are under `Assets/SpeechSDK` and `Assets/i5 Toolkit for Unity Speech Module Plugin` folder for Azure and native recognizers/synthesizers, respectively. After the importing, the package file will be deleted automatically.

If you don't want to use a specific recognizer/synthesizer anymore, you should manually delete the corresponding scripting symbol from the custom scripting symbols after you deleted the third-party packages. You can find those symbols in _PlayerSettings - Other Settings - Scripting Define Symbols_.
  
## What You Should Notice
- Don't subscribe to the events or setting the properties of `SpeechProvider` in `Awake()`, since it might haven't initialized all recognizers and synthesizers due to the execution order of scripts.
- Although some recognizer don't require a manually stop, e.g. the `AzureSpeechRecognizer` on the SingleShot mode, it is still a good choice to add a stop button on the UI and call the `StopRecordingAsync()` method of the `SpeechProvider`. When you implement such a recognizer, you can just leave the `StopRecordingAsync()` method empty.
- If you are quite sure about your use cases and only want to use one recognizer/synthesizer, you can also omit the `SpeechProvider` and directly interact with the recognizer/synthesizer.
- `PrimaryAudioOutputForm` and `Language` properties of `SpeechProvider` may not influence all recognizers or syntheisizers because they may not support them.
- Although the methods for recognizing and synthesizing do have return values, they are not guaranteed to be meaningful. In facts, they are meaningless in most cases and should only be used for `await`. Instead, you should subscribe to the `OnRecognitionResultReceived` and `OnSynthesisResultReceived` events to deal with the results.
- The neural-network models for the `NativeSpeechRecognizer` must be stored in the `StreamingAssets` folder, because they would be decompressed to the `PersistentDataPath` on the first start, so they need to be "as is" after build and shouldn't be compressed by Unity.
- The `NativeSpeechSyntheizer` only works with `Mono` Backend and `.NET 4.x` API compatibility level on Windows Standalone. For Android, it must be built with an API level greater or equal than 21.
- Although some third-party libraries contain plugins for other platforms, e.g. MacOS or IOS, they are removed, but you can still find them on the corresponding websites.